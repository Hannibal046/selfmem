## path
data_dir: ../data/wikitext103
pretrained_model_path: ../pretrained_model/bart_base
default_root_dir: /tmp
memory_dir: null
memory_encoding: concate
src: context
trg: target
## training
per_device_train_batch_size: 8
accumulate_grad_batches: 1
per_device_eval_batch_size: 12
lr: 5.0e-3
warmup_steps: 4000
accelerator: gpu
max_epochs: 10
val_check_interval: 1.0 
label_smoothing_factor: 0.1
gradient_clip_val: 1.0 
weight_decay: 0
train_max_src_len: 512
train_max_trg_len: 128
logging_steps: 100
eval_metrics: ppl
seed: 980406
## generation
gen_max_len: null
num_beams: null
